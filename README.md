# Autism-detection-using-Explainable-AI
ðŸ“‹ Overview

This project focuses on detecting Autism Spectrum Disorder (ASD) using machine learning and explainable AI (XAI) techniques. The goal is not only to achieve high prediction accuracy but also to understand and visualize the reasoning behind model decisions â€” making the system more transparent, trustworthy, and interpretable for medical experts and researchers.

ðŸŽ¯ Objectives

Develop an AI model capable of identifying autism patterns from behavioral and diagnostic datasets.

Use Explainable AI techniques (like LIME, SHAP, or Grad-CAM) to interpret model outputs.

Improve the transparency of predictions to assist healthcare professionals in clinical decision-making.

ðŸ§© Features

Data Preprocessing: Cleaning, feature selection, and normalization of ASD datasets.

Model Training: Classification using machine learning algorithms (e.g., Random Forest, SVM, Neural Networks).

Explainability Layer: Integration of explainable AI tools to visualize feature importance and decision boundaries.

Performance Evaluation: Metrics such as accuracy, precision, recall, F1-score, and ROC-AUC.

ðŸ§  Technologies Used

Python (NumPy, Pandas, Scikit-learn, TensorFlow/PyTorch)

Explainable AI Libraries: SHAP, LIME, Grad-CAM

Visualization: Matplotlib, Seaborn

Jupyter Notebook / Google Colab

ðŸ“Š Results

Achieved high accuracy in autism detection through optimized model training.

Explainable AI techniques provided insightful visual explanations for each prediction, helping validate AI decisions in a clinical context.

ðŸ’¡ Impact

By integrating Explainable AI, this project bridges the gap between black-box AI models and human understanding, contributing toward ethical, transparent, and interpretable healthcare AI systems.

ðŸš€ Future Scope

Integration with real-time diagnostic systems.

Use of multimodal data (e.g., facial expressions, speech, eye-tracking).

Deployment as a web application for clinical use.
